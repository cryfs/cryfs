name: 'S3 Cache'
description: 'S3 Cache'
inputs:
  action:
    description: "`load` or `save` the cache"
    required: true
  paths:
    description: "Directory paths to cache"
    required: true
  cache_key:
    description: "Cache key"
    required: true
  secret_aws_access_key_id:
    description: "AWS access key ID"
    required: false
  secret_aws_secret_access_key:
    description: "AWS secret access key"
    required: false
runs:
  using: "composite"
  steps:
    - name: Zip cache files
      if: ${{ inputs.action == 'save' }}
      shell: bash
      run: |
        mkdir -p "/tmp/s3_cache/${{inputs.cache_key}}"
        tar --zstd --create -Pp --same-owner --file "/tmp/s3_cache/${{inputs.cache_key}}/cache.tar.zstd" ${{ inputs.paths }}
        echo "Zipped cache:"
        ls -lh "/tmp/s3_cache/${{inputs.cache_key}}"
    - name: Store cache
      # note: this access key has write access to the cache. This can't run on PRs.
      if: ${{ inputs.action == 'save' && github.event_name == 'push' }}
      # Cache things sometimes indeterministically fail (roughly 1% of times this is run), let's not fail the job for it
      continue-on-error: true
      uses: leroy-merlin-br/action-s3-cache@7fad0a81b31884660211f24b62e29b4777a6ac4c # v1.0.6
      with:
        action: put
        aws-access-key-id: ${{ inputs.secret_aws_access_key_id }}
        aws-secret-access-key: ${{ inputs.secret_aws_secret_access_key }}
        aws-region: eu-west-1
        bucket: ci-cache.cryfs
        key: v2-${{ inputs.cache_key }}
        artifacts: "/tmp/s3_cache/${{inputs.cache_key}}"


    - name: Retrieve cache
      if: ${{ inputs.action == 'load' }}
      # Many jobs access the cache in parallel an we might observe an incomplete state that isn't valid. This would fail with a checksum error. Let's not fail the CI job but continue it, later on this job will upload a new new cache as part of the regular job run.
      continue-on-error: true
      # We're using an S3 based cache because the standard GitHub Action cache (actions/cache) only gives us 5GB of storage and we need more
      uses: leroy-merlin-br/action-s3-cache@7fad0a81b31884660211f24b62e29b4777a6ac4c # v1.0.6
      with:
        action: get
        # note: this access key has read-only access to the cache. It's public so it runs on PRs.
        aws-access-key-id: AKIAV5S2KH4F5OUZXV5E
        aws-secret-access-key: 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
        aws-region: eu-west-1
        bucket: ci-cache.cryfs
        key: v2-${{ inputs.cache_key }}
    - name: Unzip cache files
      if: ${{ inputs.action == 'load' }}
      # If the cache is corrupted, we still want to continue. Later, this job (if successful) will upload a new cache.
      continue-on-error: true
      shell: bash
      run: |
        if [ -f "/tmp/s3_cache/${{inputs.cache_key}}/cache.tar.zstd" ]; then
          echo "Zipped cache:"
          ls -lh "/tmp/s3_cache/${{inputs.cache_key}}"
          tar --extract --zstd -Pp --same-owner --file "/tmp/s3_cache/${{inputs.cache_key}}/cache.tar.zstd" ${{ inputs.paths }}
        else
          echo "Warning: Cache not found"
        fi
