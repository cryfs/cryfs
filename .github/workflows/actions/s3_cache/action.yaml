name: 'S3 Cache'
description: 'S3 Cache'
inputs:
  action:
    description: "`load` or `save` the cache"
    required: true
  paths:
    description: "Directory paths to cache"
    required: true
  cache_key:
    description: "Cache key"
    required: true
  secret_aws_access_key_id:
    description: "AWS access key ID"
    required: false
  secret_aws_secret_access_key:
    description: "AWS secret access key"
    required: false
runs:
  using: "composite"
  steps:
    - name: Zip cache files
      if: ${{ inputs.action == 'save' }}
      shell: bash
      run: |
        mkdir -p "/tmp/s3_cache/${{inputs.cache_key}}"
        for path in ${{ inputs.paths }}; do
          if [ ! -d $path ]; then
            echo "Warning: $path not found so cannot cache it"
            exit 0
            # TODO exit 1
          else
            echo "Loaded size of $path:"
            du -sh $path
          fi
        done
        tar --zstd --create -Pp --same-owner --file "/tmp/s3_cache/${{inputs.cache_key}}/cache.tar.zstd" ${{ inputs.paths }}
        echo "Zipped cache:"
        ls -lh "/tmp/s3_cache/${{inputs.cache_key}}"
    - name: Store cache
      # note: this access key has write access to the cache. This can't run on PRs.
      if: ${{ inputs.action == 'save' && github.event_name == 'push' }}
      # Cache things sometimes indeterministically fail (roughly 1% of times this is run), let's not fail the job for it
      continue-on-error: true
      uses: leroy-merlin-br/action-s3-cache@7fad0a81b31884660211f24b62e29b4777a6ac4c # v1.0.6
      with:
        action: put
        # TODO Test this works in PRs where the secrets aren't available
        aws-access-key-id: ${{ inputs.secret_aws_access_key_id }}
        aws-secret-access-key: ${{ inputs.secret_aws_secret_access_key }}
        aws-region: eu-west-1
        bucket: ci-cache.cryfs
        key: v2-${{ inputs.cache_key }}
        artifacts: "/tmp/s3_cache/${{inputs.cache_key}}"


    - name: Retrieve cache
      if: ${{ inputs.action == 'load' }}
      # Many jobs access the cache in parallel an we might observe an incomplete state that isn't valid. This would fail with a checksum error. Let's not fail the CI job but continue it, later on this job will upload a new new cache as part of the regular job run.
      continue-on-error: true
      # We're using an S3 based cache because the standard GitHub Action cache (actions/cache) only gives us 5GB of storage and we need more
      uses: leroy-merlin-br/action-s3-cache@7fad0a81b31884660211f24b62e29b4777a6ac4c # v1.0.6
      with:
        action: get
        # note: this access key has read-only access to the cache. It's public so it runs on PRs.
        aws-access-key-id: AKIAV5S2KH4F5OUZXV5E
        aws-secret-access-key: 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
        aws-region: eu-west-1
        bucket: ci-cache.cryfs
        key: v2-${{ inputs.cache_key }}
    - name: Unzip cache files
      if: ${{ inputs.action == 'load' }}
      shell: bash
      run: |
        if [ -f "/tmp/s3_cache/${{inputs.cache_key}}/cache.tar.zstd" ]; then
          echo "Zipped cache:"
          ls -lh "/tmp/s3_cache/${{inputs.cache_key}}"
          tar --extract --zstd -Pp --same-owner --file "/tmp/s3_cache/${{inputs.cache_key}}/cache.tar.zstd" ${{ inputs.paths }}
          for path in ${{ inputs.paths }}; do
            if [ ! -d $path ]; then
              echo "Warning: $path not found"
            else
              echo "Loaded size of $path:"
              du -sh $path
            fi
          done
        else
          echo "Warning: Cache not found"
        fi